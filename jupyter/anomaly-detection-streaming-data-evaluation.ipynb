{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/leomauro/seasonal-anomaly-detection-streaming-data?scriptVersionId=145158398\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Anomaly Detection - Streaming Data\n\nThis notebook presents a few unsupervised algorithms to detect anomaly in real streaming data, such as seasonal KPIs. \"Streaming data is data that is continuously generated by different sources. Such data should be processed incrementally using stream processing techniques without having access to all of the data\" [Wikipedia](https://en.wikipedia.org/wiki/Streaming_data). We are going to experiment Anomaly Detection in [Benchmark: Labeled Anomaly Detection TS](https://www.kaggle.com/caesarlupum/benchmark-labeled-anomaly-detection-ts). In the end, I hope you are going to be able to apply this family of algorithms.\n\n\n> **Summary** - Unsupervised Anomaly Detection in real Streaming Data.   \n> Content for intermediate level in Machine Learning and Data Science!   \n\n**Notes**\n- This notebook is an extension of [Anomaly Detection - Streaming Data](https://www.kaggle.com/leomauro/anomaly-detection-streaming-data/)   \n- See another extension with many other algorithms [Anomaly Detection - Streaming Data (Extended)](https://www.kaggle.com/neomatrix369/anomaly-detection-streaming-data-extended/)\n\n<a id='ToC'></a>\n## Table of Contents\n- [Data Exploration](#data)\n- [Streaming Anomaly Detection](#anomaly)\n    - [Moving Average - MA](#ma)\n    - [Exponential Moving Average - EMA](#ema)\n    - [Seasonal Moving Average - SMA](#sma)\n    - [Auto Regressive Integrated Moving Average - ARIMA](#arima)\n- [Conclusion](#conclusion)\n\n---\n<h3><a href=\"https://medium.com/wearesinch/simple-anomaly-detection-algorithms-for-streaming-data-machine-learning-92cfaeb6f43b\">Click here to read to the Medium blog post</a></h3>\n\n![](https://miro.medium.com/max/1400/1*Kh3wXbIA2oKHc8qiIy3v0g.png)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"data\"></a>\n\n---\n# Data Exploration\n\nWe are using [Benchmark: Labeled Anomaly Detection TS](https://www.kaggle.com/caesarlupum/benchmark-labeled-anomaly-detection-ts) data source that presents a corpus of four seasonal KPIs over time, designed to provide data for research in streaming anomaly detection. Also, in this section, we are going to explore the data using the visualizations proposed by [Anomaly Detection - Visualizations](https://www.kaggle.com/docxian/anomaly-detection-visualizations) notebook.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datetime as dt\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:23:41.717748Z","iopub.execute_input":"2023-10-04T01:23:41.71851Z","iopub.status.idle":"2023-10-04T01:23:42.25013Z","shell.execute_reply.started":"2023-10-04T01:23:41.718446Z","shell.execute_reply":"2023-10-04T01:23:42.24951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-04T01:23:42.254335Z","iopub.execute_input":"2023-10-04T01:23:42.254727Z","iopub.status.idle":"2023-10-04T01:23:42.274772Z","shell.execute_reply.started":"2023-10-04T01:23:42.254679Z","shell.execute_reply":"2023-10-04T01:23:42.274157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are going to use two datasets:\n\n- `/kaggle/input/benchmark-labeled-anomaly-detection-ts/g.csv`\n- `/kaggle/input/benchmark-labeled-anomaly-detection-ts/cpu4.csv`","metadata":{}},{"cell_type":"code","source":"# first dataset\ndf1 = pd.read_csv('../input/benchmark-labeled-anomaly-detection-ts/cpu4.csv')\ndf1 = df1.sort_values(by='timestamp', ascending=True)\ndf1 = df1.replace({'label': {0.0: False, 1.0: True}})\n\n# second dataset\ndf2 = pd.read_csv('../input/benchmark-labeled-anomaly-detection-ts/g.csv')\ndf2 = df2.sort_values(by='timestamp', ascending=True)\ndf2 = df2.replace({'label': {0.0: False, 1.0: True}})\n\ndf1.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:23:42.275782Z","iopub.execute_input":"2023-10-04T01:23:42.27611Z","iopub.status.idle":"2023-10-04T01:23:42.40236Z","shell.execute_reply.started":"2023-10-04T01:23:42.276081Z","shell.execute_reply":"2023-10-04T01:23:42.401516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Frequency of Anomalies\n\nHow many anomalies do we have in each dataset?","metadata":{}},{"cell_type":"code","source":"print('Frequencies:')\nprint(df1.label.value_counts(normalize=True), '\\n')\nprint(df2.label.value_counts(normalize=True))","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-10-04T01:23:42.405066Z","iopub.execute_input":"2023-10-04T01:23:42.405586Z","iopub.status.idle":"2023-10-04T01:23:42.424802Z","shell.execute_reply.started":"2023-10-04T01:23:42.40554Z","shell.execute_reply":"2023-10-04T01:23:42.423627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Anomalies in the Dataset\n\nLet's visualize the datasets.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,4))\nplt.title('Dataset 1')\nsns.scatterplot(data=df1, x=\"timestamp\", y=\"value\", hue=\"label\")\nplt.show()\n\nplt.figure(figsize=(12,4))\nplt.title('Dataset 2')\nsns.scatterplot(data=df2, x=\"timestamp\", y=\"value\", hue=\"label\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-04T01:23:42.426262Z","iopub.execute_input":"2023-10-04T01:23:42.426848Z","iopub.status.idle":"2023-10-04T01:23:46.978912Z","shell.execute_reply.started":"2023-10-04T01:23:42.426797Z","shell.execute_reply":"2023-10-04T01:23:46.978299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We cannot see what is happening. So let's zoom in on the data... therefore, we can clearly visualize the anomalies in the Streaming Data.\n- We are going to explore only 300 points of each dataset","metadata":{}},{"cell_type":"code","source":"df1_zoom = df1[800:1100]\ndf1_zoom_exp = df1[300:1800]\nplt.figure(figsize=(12,4))\nsns.scatterplot(data=df1_zoom, x=\"timestamp\", y=\"value\", hue=\"label\")\nplt.show()\n\ndf2_zoom = df2[1900:2200]\nplt.figure(figsize=(12,4))\nsns.scatterplot(data=df2_zoom, x=\"timestamp\", y=\"value\", hue=\"label\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-04T01:23:46.979814Z","iopub.execute_input":"2023-10-04T01:23:46.980585Z","iopub.status.idle":"2023-10-04T01:23:47.5268Z","shell.execute_reply.started":"2023-10-04T01:23:46.980552Z","shell.execute_reply":"2023-10-04T01:23:47.52589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href='#ToC'><span class=\"label label-info\" style=\"font-size: 125%\">Back to Table of Contents</span></a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"anomaly\"></a>\n\n---\n# Streaming Anomaly Detection\nDo you want to monitor your data and detect anomalies on fly? We are going to see a few algorithms for that:\n\n- Streaming Moving Average\n- Streaming Exponential Average","metadata":{}},{"cell_type":"code","source":"def plot_anomalies(df, algorithm, parameters, dumping=False, casting=None):\n    '''Plot the Streaming Data (an Anomalies)'''\n    Y = df.value\n    X = df.timestamp\n    X_pred = df.timestamp if casting is None else X.apply(casting)\n    # predict anomalies\n    model = algorithm(**parameters)\n    preds = [model.detect(i, v, dumping=True) for i, v in zip(X_pred, Y)]\n    pred, values, stds = tuple(zip(*preds))\n    # plot the results\n    plt.figure(figsize=(12,4))\n    model_name = algorithm.__name__\n    plt.title(f'Anomaly Detection - {model_name}')\n    af  = pd.DataFrame(data={'x':X, 'value':Y, 'pred':pred})\n    af2 = pd.DataFrame(data={'x':X, 'value':values, 'pred':pred, 'std': stds})\n    af2['ymin'] = af2['value'] - af2['std']\n    af2['ymax'] = af2['value'] + af2['std']\n    size = (af.pred.astype(int)+1) * 40\n    sns.scatterplot(data=af, x='x', y='value', hue='pred', s=size)\n    if dumping: plt.fill_between(af2.x, af2.ymin, af2.ymax, facecolor='green', alpha=0.2)\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-04T01:23:47.52813Z","iopub.execute_input":"2023-10-04T01:23:47.528453Z","iopub.status.idle":"2023-10-04T01:23:47.538739Z","shell.execute_reply.started":"2023-10-04T01:23:47.528412Z","shell.execute_reply":"2023-10-04T01:23:47.538183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ma\"></a>\n## Moving Average\n\n**Moving Average** is the most common type of average used in Time Series problems. We perform the sum of recent data points and divide them by the time period. Further, we simply check if the new record is far from the expected value. The expected value range is computed using the formula `Moving Average + standard deviation * threshold`; if it is out of the expected value range, we report as an anomaly.","metadata":{}},{"cell_type":"code","source":"class StreamingMovingAverage:\n    '''Moving Average algorithm'''\n    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.rolling.html\n\n    def __init__(self, threshold=1.0) -> None:\n        # Parameters\n        self.max_deviation_from_expected = threshold\n        self.min_nof_records_in_model = 3\n        self.max_nof_records_in_model = 3 * self.min_nof_records_in_model\n\n    def detect(self, timestamp: int, value: float, dumping: bool=False) -> bool:\n        '''Detect if is a Anomaly'''\n        self._update_state(timestamp, value)\n        expected_value = self._expected_value(timestamp)\n        # is there enough data and is not NaN value\n        response, curr_value, deviation = False, value, 0.0\n        if self._enough_data() and not np.isnan(expected_value):\n            # is the value out of the boundary? when it decrease\n            curr_value = expected_value\n            deviation = self._standard_deviation() * self.max_deviation_from_expected\n            # when it is higher than expected\n            if expected_value + deviation < value or\\\n               expected_value - deviation > value:\n                response = True\n        # dumping or not\n        if dumping: return (response, curr_value, deviation)\n        else: return response\n\n    def _update_state(self, timestamp: int, value: float) -> None:\n        '''Update the model state'''\n        # check if it is the first time the model is run or if there is a big interval between the timestamps\n        if not hasattr(self, 'previous_timestamp'):\n            self._init_state(timestamp)\n        # update the model state\n        self.previous_timestamp = timestamp\n        self.data_streaming.append(value)\n        # is there a lot of data? remove one record\n        if len(self.data_streaming) > self.max_nof_records_in_model:\n            self.data_streaming.pop(0)\n\n    def _init_state(self, timestamp: int) -> None:\n        '''Reset the parameters'''\n        self.previous_timestamp = timestamp\n        self.data_streaming = list()\n\n    def _enough_data(self) -> bool:\n        '''Check if there is enough data'''\n        return len(self.data_streaming) >= self.min_nof_records_in_model\n\n    def _expected_value(self, timestamp: int) -> float:\n        '''Return the expected value'''\n        data = self.data_streaming\n        data = pd.Series(data=data, dtype=float)\n        many = self.min_nof_records_in_model\n        return data.rolling(many, min_periods=1).mean().iloc[-1]\n\n    def _standard_deviation(self) -> float:\n        '''Return the standard deviation'''\n        data = self.data_streaming\n        return np.std(data, axis=0)\n\n    def get_state(self) -> dict:\n        '''Get the state'''\n        self_dict = {key: value for key, value in self.__dict__.items()}\n        return pickle.dumps(self_dict, 4)\n\n    def set_state(self, state) -> None:\n        '''Set the state'''\n        _self = self\n        ad = pickle.loads(state)\n        for key, value in ad.items():\n            setattr(_self, key, value)\n\n# Example\n# ad = StreamingMovingAverage(threshold=1.5)\n# ad.detect(timestamp=123, value=10)\n# ad.detect(timestamp=124, value=7)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-10-04T01:23:47.539734Z","iopub.execute_input":"2023-10-04T01:23:47.540479Z","iopub.status.idle":"2023-10-04T01:23:47.562461Z","shell.execute_reply.started":"2023-10-04T01:23:47.540427Z","shell.execute_reply":"2023-10-04T01:23:47.561619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploring the Data\n\nYou can note that the **Moving Average** is too sensitive to data. Any abrupt change it is notified as _anomaly_. In this sense, for each streaming data, you have to fit the `threshold`. However, remember thar a high `threshold` cannot find any anomaly. \n\nAlso, we can note that when the data behaviour change, the algorithm is able to quickly adapt and learn the new behavior. Another advantage of this algorithm is it only notify the anomaly once, and the sequential anomaly data (with similar behavior) is ignored due to the sensibility of the algorithm, avoiding multiples \"anomaly alerts\".","metadata":{}},{"cell_type":"code","source":"parameters = {'threshold': 1.5}\nplot_anomalies(df1_zoom, StreamingMovingAverage, parameters, dumping=True)\n\nparameters = {'threshold': 1.5}\nplot_anomalies(df2_zoom, StreamingMovingAverage, parameters, dumping=True)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-10-04T01:23:47.563917Z","iopub.execute_input":"2023-10-04T01:23:47.564402Z","iopub.status.idle":"2023-10-04T01:23:48.379384Z","shell.execute_reply.started":"2023-10-04T01:23:47.56436Z","shell.execute_reply":"2023-10-04T01:23:48.378628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href='#ToC'><span class=\"label label-info\" style=\"font-size: 125%\">Back to Table of Contents</span></a>","metadata":{}},{"cell_type":"markdown","source":"-----\n<a id=\"ema\"></a>\n\n## Exponential Moving Average\n\n**Exponential Moving Average** focuses more on recent data by assigning more weight to new data points; so, they are weigted by timestamp - most recent has more importance. Further, we simple check if the new record is far from the expected value. The expected value range is computed using the formula `Exponential Moving Average + standard deviation * threshold`; if it is out of the expected value range, we report as an anomaly.\n\nAlso, the Exponential Moving Average needs a parameter called `alpha` that determines the importance of the last record, and its value is decreased for the next records. For example, if `alpha=0.5`: the record-1 has 50% of importance, the record-2 has 30% of importance, and so on. Thus, this weighted algorithm enables smooth the expected value.","metadata":{}},{"cell_type":"code","source":"class StreamingExponentialMovingAverage(StreamingMovingAverage):\n    '''Exponential Weighted Moving Average (EWMA) algorithm'''\n    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ewm.html\n\n    def __init__(self, threshold=1.0, alpha=0.3) -> None:\n        super().__init__()\n        # Parameters\n        self.max_deviation_from_expected = threshold\n        self.alpha = alpha\n\n    def _enough_data(self) -> bool:\n        '''Check if there is enough data'''\n        return len(self.data_streaming) > 0\n\n    def _expected_value(self, timestamp: int) -> float:\n        '''Return the expected value'''\n        data = self.data_streaming\n        data = pd.Series(data=data, dtype=float)\n        return data.ewm(alpha=self.alpha, adjust=True).mean().iloc[-1]\n\n# Example\n# ad = StreamingExponentialMovingAverage(threshold=1.5, alpha=0.45)\n# ad.detect(timestamp=123, value=10)\n# ad.detect(timestamp=124, value=7)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-10-04T01:23:48.380567Z","iopub.execute_input":"2023-10-04T01:23:48.381219Z","iopub.status.idle":"2023-10-04T01:23:48.388346Z","shell.execute_reply.started":"2023-10-04T01:23:48.381185Z","shell.execute_reply":"2023-10-04T01:23:48.387471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploring the Data\n\nYou can note that the **Exponential Moving Average** is more smooth to changes than the simple **Moving Average**. However, this algorithm has two parameters to be defined by the user, they are `threshold` and `alpha`.\n\nAgain, we can note that when the data behaviour change, the algorithm is able to quickly adapt and learn the new behavior. Another advantage of this algorithm is it only notify the anomaly once, and the sequential anomaly data (with similar behavior) is ignored due to the sensibility of the algorithm, avoiding multiples \"anomaly alerts\".","metadata":{}},{"cell_type":"code","source":"parameters = {'threshold': 1.5, 'alpha': 0.45}\nplot_anomalies(df1_zoom, StreamingExponentialMovingAverage, parameters, dumping=True)\n\nparameters = {'threshold': 1.5, 'alpha': 0.45}\nplot_anomalies(df2_zoom, StreamingExponentialMovingAverage, parameters, dumping=True)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-10-04T01:23:48.389702Z","iopub.execute_input":"2023-10-04T01:23:48.389913Z","iopub.status.idle":"2023-10-04T01:23:49.229427Z","shell.execute_reply.started":"2023-10-04T01:23:48.389887Z","shell.execute_reply":"2023-10-04T01:23:49.228498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href='#ToC'><span class=\"label label-info\" style=\"font-size: 125%\">Back to Table of Contents</span></a>","metadata":{}},{"cell_type":"markdown","source":"-----\n<a id=\"sma\"></a>\n\n## Seasonal Moving Average\n\nAs for the **Seasonal Moving Average**, it's a time-based algorithm that compares specific days of the week over a period of time to identify patterns or trends. This technique is particularly useful in situations where there are regular seasonal fluctuations in the data, such as daily or weekly cycles. However, this method requires a sufficient amount of historical data to be effective, and it may not be suitable for our sample data, which is not seasonal and has a limited number of data points.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datetime as dt\n\nfrom typing import Union\n\nMINUTES_HOUR = 60 # minutes per hour\nMINUTES_DAY = 24 * MINUTES_HOUR # minutes per day\nNUMBER_DAYS_WEEK = 7 # number days a week\n\n\nclass AnomalyDetectionSeasonalBucket(StreamingMovingAverage):\n\n    def __init__(self, min_buckets: int, window: int=4, min_value: int=10, threshold: float=2) -> None:\n        '''Min number of messages is 1 per minute'''\n        super().__init__()\n        # parameters\n        self.min_buckets = min_buckets\n        self._num_bucket = int(np.floor(MINUTES_DAY / min_buckets))\n        self.window_size = window\n        self.min_value = min_value\n        self.max_deviation_from_expected = threshold\n\n    def _get_cday_cbucket(self, timestamp: dt.datetime) -> tuple:\n        '''Get cday, cbucket values from timestamp'''\n        # compute bucket indexes\n        cday, chour, cminute = timestamp.weekday(), timestamp.hour, timestamp.minute\n        cbucket = int(np.floor((chour * MINUTES_HOUR + cminute) / self.min_buckets))\n        return cday, cbucket\n    \n    def _create_copy_data(self, timestamp: dt.datetime) -> np.array:\n        '''Get data based on timestamp'''\n        # compute bucket indexes, get data\n        cday, cbucket = self._get_cday_cbucket(timestamp = timestamp)\n        data = np.copy(self._buckets[cday, cbucket])\n        data[data == 0.0] = np.nan\n        return data\n    \n    def _update_state(self, timestamp: dt.datetime, value: float) -> None:\n        '''Update the model state'''\n        # check if it is the first run\n        if not hasattr(self, 'previous_timestamp'):\n            self._init_state(timestamp)\n        # compute bucket indexes\n        cday, cbucket = self._get_cday_cbucket(timestamp = timestamp)\n        # shift values, empty days\n        pass_days = int(np.ceil((timestamp - self._buckets_timestamp[cday, cbucket]).days / NUMBER_DAYS_WEEK))\n        self._buckets[cday, cbucket] = np.roll(self._buckets[cday, cbucket], pass_days)\n        self._buckets[cday, cbucket, 0:pass_days] = 0\n        # update the model state\n        self.previous_timestamp = timestamp\n        self._buckets_timestamp[cday, cbucket] = timestamp\n        self._buckets[cday, cbucket, 0] = self._buckets[cday, cbucket, 0] + value\n\n    def _init_state(self, timestamp: dt.datetime) -> None:\n        '''Reset the parameters'''\n        self.previous_timestamp = timestamp\n        self._buckets = np.zeros((NUMBER_DAYS_WEEK, self._num_bucket, self.window_size))\n        self._buckets_timestamp = np.full((NUMBER_DAYS_WEEK, self._num_bucket), timestamp, dtype=dt.datetime)\n\n    def _expected_value(self, timestamp: dt.datetime) -> float:\n        '''Return the expected value'''\n        data = self._create_copy_data(timestamp = timestamp)\n        return np.nanmean(data)\n\n    def _enough_data(self) -> bool:\n        '''Check if there is enough data'''\n        data = self._create_copy_data(timestamp = self.previous_timestamp)\n        records = self.window_size - np.sum(np.isnan(data))\n        return records >= min(self.min_nof_records_in_model, self.window_size)\n\n    def _standard_deviation(self) -> float:\n        '''Return the standard deviation'''\n        # compute bucket indexes, get data\n        data = self._create_copy_data(timestamp = self.previous_timestamp)\n        return np.nanstd(data)\n    \n# Example\n# ad = AnomalyDetectionSeasonalBucket(min_buckets=60, window=5)\n# ad.detect(timestamp=dt.datetime(2023, 1, 23, 23, 40), value=10)\n# ad.detect(timestamp=dt.datetime(2023, 1, 30, 23, 40), value=7)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-10-04T01:23:49.230899Z","iopub.execute_input":"2023-10-04T01:23:49.231209Z","iopub.status.idle":"2023-10-04T01:23:49.249785Z","shell.execute_reply.started":"2023-10-04T01:23:49.231167Z","shell.execute_reply":"2023-10-04T01:23:49.248599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploring the Data\n\n**Seasonal Moving Average** is a time-based algorithm that relies on previous record data to make predictions, latest `window` days. It is particularly useful when analyzing patterns in seasonal data, such as weather or holiday sales. However, it is not suitable for our sample data as it does not exhibit any seasonal patterns and lacks enough days of data for analysis.\n\nIn contrast, the Exponential Moving Average is a more versatile algorithm that can adapt to changes in data behavior. It produces a smoother curve that is less affected by sudden outliers. To use this algorithm, two parameters must be defined by the user: threshold and alpha.","metadata":{}},{"cell_type":"code","source":"parameters = {'min_buckets': 30, 'window': 2}\nplot_anomalies(df1_zoom, AnomalyDetectionSeasonalBucket, parameters, dumping=True, casting=dt.datetime.fromtimestamp)\n\nparameters = {'min_buckets': 30, 'window': 2}\nplot_anomalies(df2_zoom, AnomalyDetectionSeasonalBucket, parameters, dumping=True, casting=dt.datetime.fromtimestamp)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:23:49.253136Z","iopub.execute_input":"2023-10-04T01:23:49.253824Z","iopub.status.idle":"2023-10-04T01:23:49.886023Z","shell.execute_reply.started":"2023-10-04T01:23:49.253773Z","shell.execute_reply":"2023-10-04T01:23:49.885184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When visualizing a larger set of data, the presence of anomalies may become more apparent. However, it is important to keep in mind that the accuracy of anomaly predictions heavily depends on the nature and patterns of the data being analyzed.\n\nFor instance, if the data being analyzed is not seasonal, then the predictions made by a seasonal algorithm may not be accurate. This is because seasonal algorithms rely on the assumption that certain patterns in the data repeat over time, which is not the case for non-seasonal data.","metadata":{}},{"cell_type":"code","source":"parameters = {'min_buckets': 30, 'window': 2}\nplot_anomalies(df1.iloc[0:2050], AnomalyDetectionSeasonalBucket, parameters, dumping=True, casting=dt.datetime.fromtimestamp)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:23:49.887583Z","iopub.execute_input":"2023-10-04T01:23:49.888037Z","iopub.status.idle":"2023-10-04T01:23:50.593225Z","shell.execute_reply.started":"2023-10-04T01:23:49.88799Z","shell.execute_reply":"2023-10-04T01:23:50.592316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href='#ToC'><span class=\"label label-info\" style=\"font-size: 125%\">Back to Table of Contents</span></a>","metadata":{}},{"cell_type":"markdown","source":"-----\n<a id=\"arima\"></a>\n\n## Auto Regressive Integrated Moving Average - ARIMA\n\n**ARIMA** is an acronym that stands for AutoRegressive Integrated Moving Average. It is a generalization of the simpler AutoRegressive Moving Average and adds the notion of integration. ARIMA is capable of predict values according to its previous observations. Further, we simple check if the new record is far from the expected value. The expected value range is computed using the formula `ARIMA + standard deviation * threshold`; if it is out of the expected value range, we report as an anomaly.\n\n> To know more about, I recommend these [article](https://towardsdatascience.com/machine-learning-part-19-time-series-and-autoregressive-integrated-moving-average-model-arima-c1005347b0d7) and [article](https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/)\n\nNote, the ARIMA requires a parameter called `order = (p, d, q)`, they are:\n- `p`: number of autoregressive terms (AR order)\n- `d`: number of nonseasonal differences (differencing order)\n- `q`: number of moving-average terms (MA order)","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install statsmodels==0.13.1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-04T01:23:50.594675Z","iopub.execute_input":"2023-10-04T01:23:50.595057Z","iopub.status.idle":"2023-10-04T01:24:00.311172Z","shell.execute_reply.started":"2023-10-04T01:23:50.595005Z","shell.execute_reply":"2023-10-04T01:24:00.309798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nfrom statsmodels.tsa.arima.model import ARIMA\n\nwarnings.filterwarnings('ignore')\n\nclass StreamingARIMA(StreamingMovingAverage):\n    '''ARIMA algorithm'''\n\n    def __init__(self, threshold=1.0, order:tuple=(2, 1, 2)) -> None:\n        # Parameters\n        self.order = order\n        self.model = None\n        self.res = None\n        self.max_deviation_from_expected = threshold\n        self.min_nof_records_in_model = 10\n        self.max_nof_records_in_model = 3 * self.min_nof_records_in_model\n\n    def _expected_value(self, timestamp: int) -> float:\n        '''Return the expected value'''\n        if self._enough_data():\n            data = self.data_streaming\n            self.model = ARIMA(data, order=self.order)\n            self.res = self.model.fit()\n            output = self.res.forecast()\n            return output[0]\n        return np.nan\n\n    def _standard_deviation(self) -> float:\n        '''Return the standard deviation'''\n        much = self.min_nof_records_in_model\n        data = self.data_streaming\n        if len(data) > much:\n            data = data[-much:]\n        return np.std(data, axis=0)\n\n    def summary(self) -> None:\n        '''Print the ARIMA summary'''\n        if pd.notnull(self.res):\n            print(self.res.summary())","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:24:00.313903Z","iopub.execute_input":"2023-10-04T01:24:00.314498Z","iopub.status.idle":"2023-10-04T01:24:00.638689Z","shell.execute_reply.started":"2023-10-04T01:24:00.31445Z","shell.execute_reply":"2023-10-04T01:24:00.637881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Is ARIMA accurate?\n\nLet's see the `expected value` of the ARIMA for some data points and the error `RMSE` (Root-mean-square error).","metadata":{}},{"cell_type":"code","source":"from math import sqrt\nfrom sklearn.metrics import mean_squared_error\n\n# walk-forward validation\nX = df1_zoom.iloc[0:50].value.tolist()\ntests, predictions = list(), list()\nmodel = StreamingARIMA()\n\nfor t in range(len(X)):\n    obs = X[t]\n    model.detect(t, obs)\n    yhat = model._expected_value(0)\n    if pd.notna(yhat):\n        predictions.append(yhat)\n        tests.append(obs)\n# evaluate forecasts\nrmse = sqrt(mean_squared_error(tests, predictions))\n\nprint('Test RMSE: %.3f' % rmse)\n# plot forecasts against actual outcomes\nplt.plot(tests, label='data')\nplt.plot(predictions, color='gray', label='ARIMA')\nplt.legend()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-04T01:24:00.639787Z","iopub.execute_input":"2023-10-04T01:24:00.639993Z","iopub.status.idle":"2023-10-04T01:24:24.516866Z","shell.execute_reply.started":"2023-10-04T01:24:00.639968Z","shell.execute_reply":"2023-10-04T01:24:24.515951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:24:24.518105Z","iopub.execute_input":"2023-10-04T01:24:24.518927Z","iopub.status.idle":"2023-10-04T01:24:24.543061Z","shell.execute_reply.started":"2023-10-04T01:24:24.51889Z","shell.execute_reply":"2023-10-04T01:24:24.542162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploring the Data\n\nYou can note that the **ARIMA** is more accurate than the previous methods. However, this algorithm has fours parameters to be defined by the user, they are `threshold` and three `orders`.\n\nIt is a amazing model, however it is harder to finetune for the data.","metadata":{}},{"cell_type":"code","source":"%%time\nparameters = {'threshold': 1.5, 'order': (2, 1, 2)}\nplot_anomalies(df1_zoom, StreamingARIMA, parameters, dumping=True)\n\nparameters = {'threshold': 1.5, 'order': (2, 1, 2)}\nplot_anomalies(df2_zoom, StreamingARIMA, parameters, dumping=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T01:24:24.544405Z","iopub.execute_input":"2023-10-04T01:24:24.545259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href='#ToC'><span class=\"label label-info\" style=\"font-size: 125%\">Back to Table of Contents</span></a>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"conclusion\"></a>\n\n---\n# Conclusion\n\n(1) We can see that **Exponential Moving Average** smooths the prediction better than the simple **Moving Average**. For the first dataset, the algorithm was able to detect the exact point of the anomaly and quickly adjust your prediction behaviour, avoiding multiples \"sequential anomaly alerts\".\n","metadata":{}},{"cell_type":"code","source":"parameters = {'threshold': 1.5}\nplot_anomalies(df1_zoom, StreamingMovingAverage, parameters, dumping=True)\n\nparameters = {'threshold': 1.5, 'alpha': 0.45}\nplot_anomalies(df1_zoom, StreamingExponentialMovingAverage, parameters, dumping=True)","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(2) Again, we can see that **Exponential Moving Average** smooths the prediction better than the simple **Moving Average**. However, the second dataset was more difficult than the first one, due to the high variation in the data. Anyway, the algorithm was able to detect the anomaly point, also produce a few false negative.\n","metadata":{}},{"cell_type":"code","source":"parameters = {'threshold': 1.5}\nplot_anomalies(df2_zoom, StreamingMovingAverage, parameters, dumping=True)\n\nparameters = {'threshold': 1.5, 'alpha': 0.45}\nplot_anomalies(df2_zoom, StreamingExponentialMovingAverage, parameters, dumping=True)","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(3) the **ARIMA** seems to be promising. However it still underperforming compared to **Exponential Moving Average**.","metadata":{}},{"cell_type":"code","source":"parameters = {'threshold': 1.5, 'alpha': 0.45}\nplot_anomalies(df1_zoom, StreamingExponentialMovingAverage, parameters, dumping=True)\n\nparameters = {'threshold': 1.5, 'order': (4, 1, 2)}\nplot_anomalies(df1_zoom, StreamingARIMA, parameters, dumping=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(4) **Seasonal Moving Average** is a time-based algorithm that relies on previous days to make predictions. It did not fit for these two sample use cases because it is small amount of non-seasonal data.\n\n(5) In summary, it is important to use the appropriate algorithm for the type of data being analyzed. In the case of non-seasonal data, algorithms such as the **Exponential Moving Average** or the **Holt-Winters** algorithm may be more suitable for anomaly detection. These algorithms are able to adapt to changes in the data and do not rely on seasonal patterns for their analysis. Related to seasonal data, you can explore **Seasonal Moving Average** or **Seasonal Exponential Moving Average**.","metadata":{}},{"cell_type":"markdown","source":"<a href='#ToC'><span class=\"label label-info\" style=\"font-size: 125%\">Back to Table of Contents</span></a>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}